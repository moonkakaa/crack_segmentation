{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "import seaborn as sns\n",
    "from albumentations import Compose, OneOf, Flip, Rotate, RandomGamma, ElasticTransform, GridDistortion, OpticalDistortion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "import random\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "# torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    np.random.seed(random_seed + worker_id)\n",
    "    random.seed(random_seed + worker_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DeepCrack\n",
    "root_dir = 'Dataset/DeepCrack' if True else 'Dataset'\n",
    "train_image_dir = f'{root_dir}/train_img'\n",
    "train_mask_dir = f'{root_dir}/train_lab'\n",
    "\n",
    "test_image_dir = f'{root_dir}/test_img'\n",
    "test_mask_dir = f'{root_dir}/test_lab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_paths = sorted([os.path.join(train_image_dir, fname) for fname in os.listdir(train_image_dir) if fname.endswith(\".jpg\") and not fname.startswith(\".\")])\n",
    "train_mask_paths = sorted([os.path.join(train_mask_dir, fname) for fname in os.listdir(train_mask_dir) if fname.endswith(\".png\") and not fname.startswith(\".\")])\n",
    "print(\"Number of train images : \", len(train_image_paths))\n",
    "print(\"Number of train masks : \", len(train_mask_paths))\n",
    "\n",
    "print()\n",
    "\n",
    "test_image_paths = sorted([os.path.join(test_image_dir, fname) for fname in os.listdir(test_image_dir) if fname.endswith(\".jpg\") and not fname.startswith(\".\")])\n",
    "test_mask_paths = sorted([os.path.join(test_mask_dir, fname) for fname in os.listdir(test_mask_dir) if fname.endswith(\".png\") and not fname.startswith(\".\")])\n",
    "print(\"Number of testing images : \", len(test_image_paths))\n",
    "print(\"Number of testing masks : \", len(test_mask_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train, valid\n",
    "train_image_files, train_mask_files = train_image_paths, train_mask_paths\n",
    "test_image_files, test_mask_files = test_image_paths, test_mask_paths\n",
    "\n",
    "print(len(train_image_files), len(train_mask_files))\n",
    "print(len(test_image_files), len(test_mask_files))\n",
    "\n",
    "img_dim=(256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from albumentations import Compose, OneOf, Flip, Rotate,  RandomBrightnessContrast , RandomGamma, ElasticTransform, GridDistortion, OpticalDistortion, RGBShift, CLAHE\n",
    "import albumentations\n",
    "\n",
    "class Generator(Dataset):\n",
    "    def __init__(self, x_set, y_set, augment=False):\n",
    "        self.x = x_set\n",
    "        self.y = y_set \n",
    "        self.img_dim = img_dim\n",
    "        self.augment = augment\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(self.img_dim),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        # self.augmentations = Compose([\n",
    "        #     Flip(p=0.7),\n",
    "        #     Rotate(p=0.7),\n",
    "        #     OneOf([\n",
    "        #         RandomBrightnessContrast(),\n",
    "        #         RandomGamma()\n",
    "        #     ], p=0.3),\n",
    "        #     OneOf([\n",
    "        #         ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n",
    "        #         GridDistortion(),\n",
    "        #         OpticalDistortion(distort_limit=2, shift_limit=0.5)\n",
    "        #     ], p=0.3),\n",
    "        # ])\n",
    "        \n",
    "        self.augmentations = Compose([\n",
    "            Flip(p=0.7),\n",
    "            Rotate(limit=90, p=0.7),\n",
    "            OneOf([\n",
    "                albumentations.HorizontalFlip(p=1),\n",
    "                albumentations.RandomRotate90(p=1),\n",
    "                albumentations.VerticalFlip(p=1)  \n",
    "            ], p=0.7),\n",
    "            RandomBrightnessContrast(p=0.3),\n",
    "            RandomGamma(gamma_limit=(80, 120), p=0.5)\n",
    "\n",
    "        ])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_x = np.array([(cv2.cvtColor(cv2.imread(self.x[idx]), cv2.COLOR_BGR2RGB))])[0]\n",
    "        img_y = np.array([(cv2.imread(self.y[idx], cv2.IMREAD_GRAYSCALE))])[0]\n",
    "\n",
    "        img_x = self.transform(img_x)\n",
    "        img_y = self.transform(img_y)\n",
    "\n",
    "        if self.augment:\n",
    "            img_x_np = img_x.permute(1, 2, 0).numpy()\n",
    "            img_y_np = img_y.permute(1, 2, 0).numpy()\n",
    "\n",
    "            augmented = self.augmentations(image=img_x_np, mask=img_y_np)\n",
    "            img_x = torch.from_numpy(augmented[\"image\"]).permute(2, 0, 1)\n",
    "            img_y = torch.from_numpy(augmented[\"mask\"]).permute(2, 0, 1)\n",
    "\n",
    "        img_y = img_y > 0\n",
    "        #print(img_y)\n",
    "        #print(img_x)\n",
    "        return img_x*255, img_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size and number of workers\n",
    "batch_size = 3\n",
    "num_workers = 0\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "train_dataset = Generator(train_image_files, train_mask_files, augment=True)\n",
    "test_dataset = Generator(test_image_files, test_mask_files, augment=False)\n",
    "\n",
    "# Create a data loader\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size, shuffle=True, worker_init_fn=seed_worker) \n",
    "test_data_loader = DataLoader(test_dataset, batch_size, shuffle=False, worker_init_fn=seed_worker) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in train_data_loader:\n",
    "    break\n",
    "\n",
    "print(i.shape)\n",
    "\n",
    "i = i.permute(0, 2,3,1)\n",
    "j = j.permute(0, 2,3,1)\n",
    "\n",
    "print(i.shape)\n",
    "print(j.shape)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(13,2.5))\n",
    "fig.suptitle('Original Images', fontsize=15)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for img, ax in zip(i[:5], axes[:5]):\n",
    "    #print(img.shape)\n",
    "    ax.imshow(img/255)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(13,2.5))\n",
    "fig.suptitle('Original Masks', fontsize=15)\n",
    "axes = axes.flatten()\n",
    "for img, ax in zip(j[:5], axes[:5]):\n",
    "    #print(img.shape)\n",
    "    ax.imshow(np.squeeze(img, -1), cmap='gray')\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.layers import trunc_normal_\n",
    "\n",
    "class DistilledVisionTransformer(VisionTransformer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 3, kernel_size=7, stride=2, padding=4),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        )\n",
    "\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, self.embed_dim))\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, self.embed_dim))\n",
    "        self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if self.num_classes > 0 else nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.dist_token, std=.02)\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        self.head_dist.apply(self._init_weights)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # taken from https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "        # with slight modifications to add the dist_token\n",
    "        B = x.shape[0]\n",
    "        x = self.conv(x)\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        dist_token = self.dist_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, dist_token, x), dim=1)\n",
    "\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0], x[:, 1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, x_dist = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        x_dist = self.head_dist(x_dist)\n",
    "        if self.training:\n",
    "            return x, x_dist\n",
    "        else:\n",
    "            # during inference, return the average of both classifier predictions\n",
    "            return (x + x_dist) / 2\n",
    "\n",
    "\n",
    "@register_model\n",
    "def deit_tiny_patch16_256(pretrained=True,**kwargs):\n",
    "    model = VisionTransformer(\n",
    "        img_size=256, patch_size=16, embed_dim=192, depth=12, num_heads=3, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    if pretrained:\n",
    "        ckpt = torch.load('pretrained/deit_tiny_patch16_256.pth')\n",
    "        model.load_state_dict(ckpt['model'], strict=False)\n",
    "    model.default_cfg = _cfg()\n",
    "    return model\n",
    "\n",
    "\n",
    "def deit_tiny_distilled_patch16_256(pretrained=True,**kwargs):\n",
    "    model = DistilledVisionTransformer(\n",
    "        img_size=256, patch_size=16, embed_dim=256, depth=12, num_heads=8, mlp_ratio=4, qkv_bias=True,\n",
    "        norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)\n",
    "    if pretrained:\n",
    "        ckpt = torch.load('pretrained/deit_tiny_distilled_patch16_256.pth')\n",
    "        model.load_state_dict(ckpt['model'], strict=False)\n",
    "    model.default_cfg = _cfg()\n",
    "    return model\n",
    "\n",
    "\n",
    "## Specifically, the images in the ImageNet 2012 dataset ( Russakovsky et al., 2015 ) were resized to 256 × 256 , which were divided into 256 patches with the resolution of 16 × 16 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "class resnet34(torch.nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        self.features = models.resnet34(pretrained=pretrained)\n",
    "        self.conv1 = self.features.conv1\n",
    "        self.bn1 = self.features.bn1\n",
    "        self.relu = self.features.relu\n",
    "        self.layer1 = self.features.layer1\n",
    "        self.layer2 = self.features.layer2\n",
    "        self.layer3 = self.features.layer3\n",
    "        self.layer4 = self.features.layer4\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x0 = self.relu(x)\n",
    "        feature1 = self.layer1(x0)  # 1 / 4\n",
    "        feature2 = self.layer2(feature1)  # 1 / 8\n",
    "        feature3 = self.layer3(feature2)  # 1 / 16\n",
    "        feature4 = self.layer4(feature3)  # 1 / 32\n",
    "\n",
    "        return x0, feature1, feature2, feature3, feature4\n",
    "\n",
    "class PyramidPoolingModule(nn.Module):\n",
    "    def __init__(self, pyramids=[1, 2, 3, 6]):\n",
    "        super(PyramidPoolingModule, self).__init__()\n",
    "        self.pyramids = pyramids\n",
    "\n",
    "    def forward(self, input):\n",
    "        feat = input\n",
    "        height, width = input.shape[2:]\n",
    "        for bin_size in self.pyramids:\n",
    "            x = F.adaptive_avg_pool2d(input, output_size=bin_size)\n",
    "            x = F.interpolate(x, size=(height, width), mode='bilinear', align_corners=True)\n",
    "            feat = feat + x\n",
    "        return feat\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, r=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // r, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // r, channel, bias=False),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        # Squeeze\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        # Excitation\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        # Fusion\n",
    "        y = torch.mul(x, y)\n",
    "        return y\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(out_channel, out_channel, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channel)\n",
    "        )\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channel)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        residual = self.shortcut(x)\n",
    "        out += residual\n",
    "        return F.relu(out)\n",
    "\n",
    "class FeatureFusion(nn.Module):\n",
    "    \"\"\"CFF Unit\"\"\"\n",
    "\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(FeatureFusion, self).__init__()\n",
    "        self.fusion = ResBlock(in_channel, out_channel)\n",
    "\n",
    "    def forward(self, x_high, x_low):\n",
    "        x_low = F.interpolate(x_low, size=x_high.size()[2:], mode='bilinear', align_corners=True)\n",
    "        x = torch.cat((x_low, x_high), dim=1)\n",
    "        x = self.fusion(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class RPMBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(RPMBlock, self).__init__()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=3, padding=1)\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels, out_channels=channels, kernel_size=1)\n",
    "\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x3 = self.conv3(x)\n",
    "        x3 = self.relu3(x3)\n",
    "        x1 = self.conv1(x)\n",
    "        x1 = self.relu1(x1)\n",
    "        out = x3 + x1\n",
    "        return out\n",
    "\n",
    "class DecoderBottleneckLayer(nn.Module):\n",
    "    def __init__(self, in_channels, n_filters, use_transpose=True):\n",
    "        super(DecoderBottleneckLayer, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels // 4, 1)\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels // 4)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        if use_transpose:\n",
    "            self.up = nn.Sequential(\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels // 4, in_channels // 4, 3, stride=2, padding=1, output_padding=1\n",
    "                ),\n",
    "                nn.BatchNorm2d(in_channels // 4),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.up = nn.Upsample(scale_factor=2, align_corners=True, mode=\"bilinear\")\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels // 4, n_filters, 1)\n",
    "        self.norm3 = nn.BatchNorm2d(n_filters)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.up(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm3(x)\n",
    "        x = self.relu3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCTCNet(nn.Module):\n",
    "    def __init__(self, n_channels=3, n_classes=1):\n",
    "        super(PCTCNet, self).__init__()\n",
    "        self.n_class = n_classes\n",
    "        self.inchannels = n_channels\n",
    "        size = 256\n",
    "\n",
    "        self.cnn = resnet34(pretrained=False)\n",
    "        self.headpool = PyramidPoolingModule()\n",
    "\n",
    "        transformer = deit_tiny_distilled_patch16_256(pretrained=False)\n",
    "        self.patch_embed = transformer.patch_embed\n",
    "        self.transformers = nn.ModuleList(\n",
    "            [transformer.blocks[i] for i in range(12)]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.se = SEBlock(channel=512)\n",
    "        self.se1 = SEBlock(channel=64)\n",
    "        self.se2 = SEBlock(channel=128)\n",
    "        self.se3 = SEBlock(channel=256)\n",
    "\n",
    "        self.fusion = FeatureFusion(in_channel=512 + size, out_channel=512)\n",
    "        self.fusion1 = FeatureFusion(in_channel=64 + size, out_channel=64)\n",
    "        self.fusion2 = FeatureFusion(in_channel=128 + size, out_channel=128)\n",
    "        self.fusion3 = FeatureFusion(in_channel=256 + size, out_channel=256)\n",
    "\n",
    "        self.RPMBlock1 = RPMBlock(channels=64)\n",
    "        self.RPMBlock2 = RPMBlock(channels=128)\n",
    "        self.RPMBlock3 = RPMBlock(channels=256)\n",
    "        self.FAM1 = nn.ModuleList([self.RPMBlock1 for i in range(6)])\n",
    "        self.FAM2 = nn.ModuleList([self.RPMBlock2 for i in range(4)])\n",
    "        self.FAM3 = nn.ModuleList([self.RPMBlock3 for i in range(2)])\n",
    "\n",
    "        filters = [64, 128, 256, 512]\n",
    "        self.decoder4 = DecoderBottleneckLayer(filters[3], filters[2])\n",
    "        self.decoder3 = DecoderBottleneckLayer(filters[2], filters[1])\n",
    "        self.decoder2 = DecoderBottleneckLayer(filters[1], filters[0])\n",
    "        self.decoder1 = DecoderBottleneckLayer(filters[0], filters[0])\n",
    "\n",
    "        self.final_conv1 = nn.ConvTranspose2d(filters[0], 32, 4, 2, 1)\n",
    "        self.final_relu1 = nn.ReLU(inplace=True)\n",
    "        self.final_conv2 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "        self.final_relu2 = nn.ReLU(inplace=True)\n",
    "        self.final_conv3 = nn.Conv2d(32, n_classes, 3, padding=1)\n",
    "        \n",
    "        # self.prev_cnn = nn.Conv2d(3, 3, kernel_size=1, padding=0)\n",
    "        self.prev_cnn = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=1, stride=1, padding=0, groups=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        e0, e1, e2, e3, e4 = self.cnn(x)\n",
    "        feature_cnn = self.headpool(e4)\n",
    "        size = 256\n",
    "\n",
    "        prev_cnn = self.prev_cnn(x)\n",
    "\n",
    "\n",
    "         # 12 layer\n",
    "        emb = self.patch_embed(prev_cnn)\n",
    "        emb = self.transformers[0](emb)\n",
    "        emb = self.transformers[1](emb)\n",
    "        emb1 = self.transformers[2](emb)\n",
    "        feature_tf1 = emb1.permute(0, 2, 1)\n",
    "        feature_tf1 = feature_tf1.view(b, size, 16, 16)\n",
    "\n",
    "        emb1 = self.transformers[3](emb1)\n",
    "        emb1 = self.transformers[4](emb1)\n",
    "        emb2 = self.transformers[5](emb1)\n",
    "        feature_tf2 = emb2.permute(0, 2, 1)\n",
    "        feature_tf2 = feature_tf2.view(b, size, 16, 16)\n",
    "\n",
    "        emb2 = self.transformers[6](emb2)\n",
    "        emb2 = self.transformers[7](emb2)\n",
    "        emb3 = self.transformers[8](emb2)\n",
    "        feature_tf3 = emb3.permute(0, 2, 1)\n",
    "        feature_tf3 = feature_tf3.view(b, size, 16, 16)\n",
    "\n",
    "        emb3 = self.transformers[9](emb3)\n",
    "        emb3 = self.transformers[10](emb3)\n",
    "        emb4 = self.transformers[11](emb3)\n",
    "\n",
    "        feature_tf = emb4.permute(0, 2, 1)\n",
    "        feature_tf = feature_tf.view(b, size, 16, 16)\n",
    "\n",
    "        feature_cat = self.fusion(feature_cnn, feature_tf)\n",
    "        feature_cat = self.se(feature_cat)\n",
    "\n",
    "        e1 = self.fusion1(e1, feature_tf1)\n",
    "        e2 = self.fusion2(e2, feature_tf2)\n",
    "        e3 = self.fusion3(e3, feature_tf3)\n",
    "        e1 = self.se1(e1)\n",
    "        e2 = self.se2(e2)\n",
    "        e3 = self.se3(e3)\n",
    "        for i in range(2):\n",
    "            e3 = self.FAM3[i](e3)\n",
    "        for i in range(4):\n",
    "            e2 = self.FAM2[i](e2)\n",
    "        for i in range(6):\n",
    "            e1 = self.FAM1[i](e1)\n",
    "\n",
    "        d4 = self.decoder4(feature_cat) + e3\n",
    "        d3 = self.decoder3(d4) + e2\n",
    "        d2 = self.decoder2(d3) + e1\n",
    "        out1 = self.final_conv1(d2)\n",
    "        out1 = self.final_relu1(out1)\n",
    "        out = self.final_conv2(out1)\n",
    "        out = self.final_relu2(out)\n",
    "        out = self.final_conv3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss & Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        #print(inputs, targets)\n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice\n",
    "class DiceFocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2., alpha=0.25, weight=None, size_average=True):\n",
    "        super(DiceFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        # Focal Loss\n",
    "        if not (targets.size() == inputs.size()):\n",
    "            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n",
    "                             .format(targets.size(), inputs.size()))\n",
    "\n",
    "        max_val = (-inputs).clamp(min=0)\n",
    "        loss = inputs - inputs * targets + max_val + ((-max_val).exp() + (-inputs - max_val).exp()).log()\n",
    "\n",
    "        invprobs = F.logsigmoid(-inputs * (targets * 2.0 - 1.0))\n",
    "        loss = (invprobs * self.gamma).exp() * loss\n",
    "        \n",
    "        focal_loss = self.alpha * (1 - targets) * loss + (1 - self.alpha) * targets * loss\n",
    "        focal_loss = focal_loss.sum()\n",
    "\n",
    "        # Dice Loss\n",
    "        inputs = torch.sigmoid(inputs) \n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice_loss = 1 - (2.*intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n",
    "        \n",
    "        return 0.5* dice_loss + 0.5*focal_loss\n",
    "        \n",
    "class DiceBCELoss(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        return 0.25 * self.bce(outputs, targets) + 0.75 * self.dice(outputs, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_test_sample = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer, scheduler):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.float().to(device), y.float().to(device)\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import json\n",
    "accuracy = torchmetrics.Accuracy(task='binary').to(device)\n",
    "precision = torchmetrics.Precision(task='binary').to(device)\n",
    "recall = torchmetrics.Recall(task='binary').to(device)\n",
    "f1= torchmetrics.F1Score(task='binary').to(device)\n",
    "Auroc = torchmetrics.AUROC(task='binary').to(device)\n",
    "IoU = torchmetrics.JaccardIndex(task='binary').to(device)\n",
    "Dice = torchmetrics.Dice().to(device)\n",
    "\n",
    "previous_metrics = [(0,0,0,0)] \n",
    "#old_model_metrics = json.load(open('../../DTrC-Net/history.json', 'r'))\n",
    "\n",
    "def test(dataloader, model, loss_fn, current_epoch):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0,0\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    index_x = 0\n",
    "    total_pred = torch.Tensor().to(device)\n",
    "    total_y = torch.Tensor().bool().to(device)\n",
    "    \n",
    "    if current_epoch % 10 == 0:\n",
    "        fig, (ax) = plt.subplots(4, max_test_sample, figsize=(40,12))\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.float().to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            pred = pred.to(device)\n",
    "            pred = torch.sigmoid(pred)\n",
    "            total_pred = torch.cat((total_pred, pred), dim=0) \n",
    "            total_y = torch.cat((total_y, y), dim=0)\n",
    "            if current_epoch % 10 == 0:\n",
    "                if index_x < max_test_sample:\n",
    "                    heatmap_np = pred.cpu().detach().numpy()\n",
    "                    heatmap_np = heatmap_np[0, 0, :, :]\n",
    "                    # nomalize\n",
    "                    heatmap_np = (heatmap_np - heatmap_np.min()) / (heatmap_np.max() - heatmap_np.min()) \n",
    "\n",
    "                    fig.colorbar(ax[0,index_x].imshow(X.cpu().detach().numpy()[0, 0, :, :]), ax=ax[0,index_x])\n",
    "                    fig.colorbar(ax[1,index_x].imshow(y.cpu().detach().numpy()[0, 0, :, :]), ax=ax[1,index_x])\n",
    "                    fig.colorbar(ax[2,index_x].imshow(heatmap_np), ax=ax[2,index_x])\n",
    "                    ax[3,2].set_title(\"> 0.5\")\n",
    "                    fig.colorbar(ax[3,index_x].imshow(heatmap_np > 0.5), ax=ax[3,index_x])\n",
    "\n",
    "                    ax[0,index_x].axis('off')\n",
    "                    ax[1,index_x].axis('off')\n",
    "                    ax[2,index_x].axis('off')\n",
    "                    ax[3,index_x].axis('off')\n",
    "                index_x += 1\n",
    "                \n",
    "    if current_epoch % 10 == 0:\n",
    "        plt.show()\n",
    "    \n",
    "    score_accuracy = accuracy(total_pred, total_y).item()\n",
    "    score_loss = loss_fn(total_pred, total_y.float()).item()\n",
    "    score_precision = precision(total_pred, total_y).item()\n",
    "    score_recall = recall(total_pred, total_y).item()\n",
    "    score_f1 = f1(total_pred, total_y).item()\n",
    "    score_IoU = IoU(total_pred, total_y).item()\n",
    "    score_Dice = Dice(total_pred, total_y).item()\n",
    "    \n",
    "    \n",
    "    print(f\"Accuracy: {(100*score_accuracy):>8f}%({100*(score_accuracy-previous_metrics[-1][0]):+g}%p)\")\n",
    "    print(f\"Loss: {score_loss:>8f}({score_loss - previous_metrics[-1][1]:+g})\")\n",
    "    print(f\"Precision: {score_precision:>8f}\")\n",
    "    print(f\"Recall: {score_recall:>8f}\")\n",
    "    print(f\"F1: {score_f1:>8f}\")\n",
    "    print(f\"IoU: {score_IoU:>8f}\")\n",
    "    print(f\"Dice: {score_Dice:>8f}\")\n",
    "\n",
    "    if current_epoch == 0:\n",
    "        previous_metrics.clear()\n",
    "    previous_metrics.append((score_accuracy, score_loss, score_IoU, score_Dice, score_precision, score_recall, score_f1))\n",
    "    print(f\"Global precision: {sum([m[4] for m in previous_metrics])/len(previous_metrics):>8f}\")\n",
    "    print(f\"Global recall: {sum([m[5] for m in previous_metrics])/len(previous_metrics):>8f}\")\n",
    "    print(f\"Global F1: {sum([m[6] for m in previous_metrics])/len(previous_metrics):>8f}\")\n",
    "    \n",
    "    if current_epoch % 10 == 0:\n",
    "        epochs = range(1, len(previous_metrics) + 1)  # Increment the upper limit by 1\n",
    "        plt.plot(epochs, [m[0] for m in previous_metrics], label='Accuracy')\n",
    "        plt.plot(epochs, [m[1] for m in previous_metrics], label='Loss')\n",
    "        plt.plot(epochs, [m[2] for m in previous_metrics], label='IoU')\n",
    "        plt.plot(epochs, [m[3] for m in previous_metrics], label='Dice')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Metrics')\n",
    "        plt.xlim([1, len(epochs)])\n",
    "        plt.xticks(range(1, len(epochs)))  # Set the ticks to match the updated range\n",
    "        plt.title('Metrics Progression')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.scatter([m[5] for m in previous_metrics], [m[4] for m in previous_metrics])\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "\n",
    "        plt.xlim([0, 1])\n",
    "        plt.ylim([0, 1])\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(epochs, [m[1] for m in previous_metrics][:len(previous_metrics)], label='Current model')\n",
    "        #plt.plot(epochs, [m[1] for m in old_model_metrics][:len(previous_metrics)], label='Old model')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.xlim([1, len(epochs)])\n",
    "        plt.xticks(range(1, len(epochs)))  # Set the ticks to match the updated range\n",
    "        plt.title('Loss comparison')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    print(\"----------------------------------------------------------------------------------\")\n",
    "    #json.dump(previous_metrics, open(f\"history.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummaryX import summary\n",
    "\n",
    "# Instantiate your model and move it to GPU\n",
    "model = PCTCNet().to(device)\n",
    "\n",
    "# Print model summary\n",
    "summary(model, torch.zeros((1, 3, 256, 256)).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# model = get_efficientunet_b7(out_channels=1, concat_input=True, pretrained=True).to(device)\n",
    "\n",
    "# inp = torch.randn(1, 3, 512, 512)\n",
    "model = PCTCNet().to(device)\n",
    "#loss_fn = nn.BCEWithLogitsLoss().to(device)\n",
    "loss_fn = DiceBCELoss().to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "# Define your optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "# Set the epoch milestones at which to adjust the learning rate\n",
    "milestones = [20, 50, 100]\n",
    "\n",
    "epochs = 300\n",
    "# Set the factor by which the learning rate will decay\n",
    "gamma = 0.1\n",
    "\n",
    "# Calculate the step size based on the number of epochs\n",
    "step_size = int(epochs / len(milestones))\n",
    "\n",
    "\n",
    "lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for current_epoch in range(epochs):\n",
    "    print(f\"Epoch {current_epoch+1}\\n-------------------------------\")\n",
    "    train(train_data_loader, model, loss_fn, optimizer,lr_scheduler)\n",
    "    test(test_data_loader, model, loss_fn, current_epoch)\n",
    "    \n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
